# video/pipeline.py
import logging
import os
from typing import Dict, Any, List, Tuple, Optional, Union
import math

import cv2
import numpy as np
import mediapipe as mp

# Audio processing imports
import time
from .audio_utils import extract_wav_mono_16k, compute_vad_segments, compute_pause_metrics
from .scoring import compute_scores
from audio.asr import transcribe_wav
from audio.text_metrics import compute_wpm, detect_spanish_fillers, normalize_fillers_per_minute
from audio.prosody import compute_prosody_metrics

logger = logging.getLogger(__name__)

# Configuration management - centralized constants
class Config:
    """Centralized configuration management for pipeline parameters."""
    
    # Gesture detection parameters
    GESTURE_MIN_AMP = float(os.getenv("SPEECHUP_GESTURE_MIN_AMP", "0.22"))
    GESTURE_MIN_DUR = float(os.getenv("SPEECHUP_GESTURE_MIN_DUR", "0.10"))
    GESTURE_COOLDOWN_S = float(os.getenv("SPEECHUP_GESTURE_COOLDOWN", "0.30"))
    REQUIRE_FACE_FOR_GEST = os.getenv("SPEECHUP_GESTURE_REQUIRE_FACE", "1") in ("1", "true", "True", "yes", "on")
    WARMUP_SEC = 0.5  # ignore first 0.5s
    MIN_EVENT_GAP_SEC = GESTURE_COOLDOWN_S  # use configurable cooldown
    HYST_LOW_MULT = float(os.getenv("SPEECHUP_GESTURE_HYST_LOW_MULT", "0.55"))
    HYST_LOW = HYST_LOW_MULT * GESTURE_MIN_AMP  # Hysteresis low threshold
    MAX_SEG_S = float(os.getenv("SPEECHUP_GESTURE_MAX_SEG_S", "2.5"))  # Safety max segment length
    LONG_PAUSE_S = float(os.getenv("SPEECHUP_LONG_PAUSE_SEC", "0.8"))
    MAX_EVENTS = int(os.getenv("SPEECHUP_MAX_EVENTS", "200"))
    
    # Transcript configuration
    INCLUDE_FULL_TRANSCRIPT = os.getenv("SPEECHUP_INCLUDE_TRANSCRIPT", "0") in ("1", "true", "True", "yes", "on")
    TRANSCRIPT_PREVIEW_MAX = int(os.getenv("SPEECHUP_TRANSCRIPT_PREVIEW_MAX", "1200"))
    
    @staticmethod
    def flag(name: str, default_on: bool = True) -> bool:
        """
        Read feature flag from env with safe defaults and cast to boolean.
        
        Args:
            name: Environment variable name
            default_on: If env var missing, treat as enabled when True
            
        Returns:
            Boolean indicating if the flag is enabled
        """
        raw = os.getenv(name)
        if raw is None:
            return default_on
        return raw.strip().lower() in ("1", "true", "yes", "on")
    
    # Audio processing flags - computed once at module import time
    USE_AUDIO = flag.__func__("SPEECHUP_USE_AUDIO", default_on=True)
    USE_ASR = flag.__func__("SPEECHUP_USE_ASR", default_on=True)
    USE_PROSODY = flag.__func__("SPEECHUP_USE_PROSODY", default_on=True)


def clamp(val: float, lo: float, hi: float) -> float:
    """Clamp value between lo and hi."""
    return max(lo, min(hi, val))


def smooth_wpm_segments(wpm_values: List[float], window_size: int = 3) -> float:
    """
    Apply moving average smoothing to WPM values.
    
    Args:
        wpm_values: List of WPM values from different segments
        window_size: Size of moving average window (default: 3)
    
    Returns:
        Smoothed WPM value
    """
    if not wpm_values:
        return 0.0
    
    if len(wpm_values) < window_size:
        # If not enough segments, return simple average
        return sum(wpm_values) / len(wpm_values)
    
    # Apply moving average using numpy for better performance
    values_array = np.array(wpm_values)
    kernel = np.ones(window_size) / window_size
    smoothed_values = np.convolve(values_array, kernel, mode='valid')
    
    # Return average of smoothed values
    return float(np.mean(smoothed_values)) if len(smoothed_values) > 0 else 0.0


def bucket_key(ts: float) -> str:
    """
    Convert timestamp to 5-second bucket key.
    
    Args:
        ts: Timestamp in seconds
        
    Returns:
        String bucket key in format "start-end"
    """
    b0 = int(ts // 5) * 5
    return f"{b0}-{b0+5}"


def compute_posture_openness(pose_landmarks, frame_width: int) -> float:
    """
    Compute posture openness based on shoulder span.
    
    Args:
        pose_landmarks: MediaPipe pose landmarks
        frame_width: Width of the video frame
        
    Returns:
        Float value between 0.0 and 1.0 representing posture openness
    """
    if not pose_landmarks:
        return 0.5  # Default neutral posture
    
    try:
        # Get shoulder landmarks
        left_shoulder = pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.RIGHT_SHOULDER]
        
        # Compute shoulder span in normalized coordinates
        shoulder_span = math.sqrt((right_shoulder.x - left_shoulder.x)**2 + 
                                 (right_shoulder.y - left_shoulder.y)**2)
        
        # Map to posture_openness via clamp: (shoulder_span_norm - 0.15) / (0.35 - 0.15)
        # This maps typical shoulder spans [0.15, 0.35] to [0, 1]
        posture_openness = clamp((shoulder_span - 0.15) / 0.2, 0.0, 1.0)
        
        return posture_openness
    except (IndexError, AttributeError) as e:
        logger.debug(f"Error computing posture openness: {e}")
        return 0.5  # Safe default


def compute_expression_variability(face_landmarks) -> float:
    """
    Compute expression variability using FaceMesh landmarks.
    
    Args:
        face_landmarks: MediaPipe face mesh landmarks
        
    Returns:
        Float value between 0.0 and 1.0 representing expression variability
    """
    if not face_landmarks:
        return 0.0
    
    try:
        # Get landmark indices for facial features
        MOUTH_LEFT = 61
        MOUTH_RIGHT = 291
        MOUTH_TOP = 13
        MOUTH_BOTTOM = 14
        LEFT_BROW = 70
        RIGHT_BROW = 300
        LEFT_EYE = 33
        RIGHT_EYE = 133
        
        # Extract landmarks
        left_corner = face_landmarks.landmark[MOUTH_LEFT]
        right_corner = face_landmarks.landmark[MOUTH_RIGHT]
        top_mouth = face_landmarks.landmark[MOUTH_TOP]
        bottom_mouth = face_landmarks.landmark[MOUTH_BOTTOM]
        
        # Compute Euclidean distances
        mouth_width = math.sqrt((right_corner.x - left_corner.x)**2 + 
                               (right_corner.y - left_corner.y)**2)
        mouth_height = math.sqrt((top_mouth.x - bottom_mouth.x)**2 + 
                                (top_mouth.y - bottom_mouth.y)**2)
        
        # Avoid division by zero
        smile_ratio = mouth_width / max(mouth_height, 0.001)
        
        # Brow raise calculation
        left_brow = face_landmarks.landmark[LEFT_BROW]
        right_brow = face_landmarks.landmark[RIGHT_BROW]
        left_eye = face_landmarks.landmark[LEFT_EYE]
        right_eye = face_landmarks.landmark[RIGHT_EYE]
        
        left_brow_raise = math.sqrt((left_brow.x - left_eye.x)**2 + 
                                   (left_brow.y - left_eye.y)**2)
        right_brow_raise = math.sqrt((right_brow.x - right_eye.x)**2 + 
                                    (right_brow.y - right_eye.y)**2)
        brow_raise = (left_brow_raise + right_brow_raise) / 2.0
        
        # Normalize and combine with weights
        smile_norm = clamp(smile_ratio / 2.0, 0.0, 1.0)  # Assuming max ratio is ~2.0
        brow_norm = clamp(brow_raise / 0.1, 0.0, 1.0)   # Assuming max distance is ~0.1
        
        # Combine with weights: 60% smile, 40% brow
        expression_variability = 0.6 * smile_norm + 0.4 * brow_norm
        
        return clamp(expression_variability, 0.0, 1.0)
        
    except (IndexError, AttributeError) as e:
        logger.debug(f"Error computing expression variability: {e}")
        return 0.0  # Safe default


class GestureDetector:
    """
    Gesture detection state machine with hysteresis.
    
    This class encapsulates the gesture detection logic, handling the state machine
    for detecting hand motion events with hysteresis, cooldown periods, and face presence
    requirements.
    """
    
    def __init__(self, config=None):
        """
        Initialize the gesture detector with configuration parameters.
        
        Args:
            config: Configuration object (uses Config class if None)
        """
        # Use provided config or default to global Config
        self.config = config or Config
        
        # Configuration parameters
        self.min_amp = self.config.GESTURE_MIN_AMP
        self.min_dur_s = self.config.GESTURE_MIN_DUR
        self.cooldown_s = self.config.GESTURE_COOLDOWN_S
        self.require_face = self.config.REQUIRE_FACE_FOR_GEST
        self.warmup_sec = self.config.WARMUP_SEC
        self.hyst_low = self.config.HYST_LOW
        self.max_seg_s = self.config.MAX_SEG_S
        
        # State variables
        self.gesture_active = False
        self.gesture_start_idx = -1
        self.gesture_face_hits = 0
        self.gesture_sum_amp = 0.0
        self.gesture_n_amp = 0
        self.gesture_last_end_t = float("-inf")
        
        # Statistics
        self.gesture_candidates = 0
        self.confirmed_events = []
        self.gesture_buckets = {}
        
        # Tracking arrays
        self.motion_magnitudes = []
        self.face_presence = []
        self.frame_timestamps = []
        
        # FPS for time calculations
        self.fps = 30.0
    
    def set_fps(self, fps):
        """Set the frames per second for time calculations."""
        self.fps = fps or 30.0
    
    def track_frame(self, delta, has_face, frame_idx):
        """
        Track a new frame for gesture detection.
        
        Args:
            delta: Motion magnitude for this frame
            has_face: Boolean indicating if a face was detected
            frame_idx: Frame index
            
        Returns:
            Boolean indicating if a gesture was detected and processed
        """
        # Compute current time in seconds for this frame
        t_sec = frame_idx / self.fps
        
        # Store motion magnitude and face presence for gesture detection
        self.motion_magnitudes.append(delta)
        self.face_presence.append(has_face)
        self.frame_timestamps.append(t_sec)
        
        # Skip if in warmup period
        if t_sec < self.warmup_sec:
            return False
        
        gesture_processed = False
        
        if not self.gesture_active:
            # Check if we should start a new gesture window
            if delta >= self.min_amp and (t_sec - self.gesture_last_end_t) >= self.cooldown_s:
                # Open window
                self.gesture_active = True
                self.gesture_start_idx = frame_idx
                self.gesture_face_hits = 1 if has_face else 0
                self.gesture_sum_amp = delta
                self.gesture_n_amp = 1
        else:
            # Accumulate in active window
            self.gesture_face_hits += 1 if has_face else 0
            self.gesture_sum_amp += delta
            self.gesture_n_amp += 1
            
            # Check if we should close the window (hysteresis)
            if delta < self.hyst_low:
                # Process the gesture candidate
                gesture_processed = self._process_gesture_candidate(frame_idx)
                
                # Reset window state
                self.gesture_active = False
                self.gesture_start_idx = -1
                self.gesture_face_hits = 0
                self.gesture_sum_amp = 0.0
                self.gesture_n_amp = 0
        
        return gesture_processed
    
    def _process_gesture_candidate(self, end_idx):
        """
        Process a gesture candidate when the window closes.
        
        Args:
            end_idx: Frame index where the gesture window closes
            
        Returns:
            Boolean indicating if a confirmed gesture was detected
        """
        self.gesture_candidates += 1
        
        # Limit maximum segment length for safety
        end_idx = min(end_idx, self.gesture_start_idx + int(self.max_seg_s * self.fps) - 1)
        duration = (end_idx - self.gesture_start_idx + 1) / self.fps
        
        # Check minimum duration requirement
        if duration < self.min_dur_s:
            return False
        
        # Face ratio check at decision time
        if self.require_face and (self.gesture_face_hits / max(1, self.gesture_n_amp)) < 0.5:
            return False
        
        start_t = self.gesture_start_idx / self.fps
        end_t = (end_idx + 1) / self.fps
        mean_amp = self.gesture_sum_amp / max(1, self.gesture_n_amp)
        
        # Duration warning for debugging
        if duration > self.max_seg_s + 0.05:
            logger.warning("Long gesture segment detected: %.2fs (start=%.1fs, end=%.1fs)", 
                         duration, start_t, end_t)
        
        # Confidence normalized above MIN_AMP
        conf = max(0.0, min(1.0, (mean_amp - self.min_amp) / max(1e-6, (0.9 - self.min_amp))))
        
        # Create event object
        event = {
            "t": float(start_t),            # backward compatibility
            "end_t": float(end_t),
            "duration": float(end_t - start_t),
            "frame": int(self.gesture_start_idx),
            "kind": "gesture",
            "label": "hand_motion",
            "amplitude": float(mean_amp),
            "score": None,
            "confidence": float(conf),
        }
        
        self.confirmed_events.append(event)
        
        # Update tracking
        self.gesture_last_end_t = end_t
        
        # Update bucket counts
        bucket_key_str = bucket_key(start_t)
        self.gesture_buckets[bucket_key_str] = self.gesture_buckets.get(bucket_key_str, 0) + 1
        
        return True
    
    def finalize(self, last_frame_idx):
        """
        Finalize gesture detection and process any active gesture window.
        
        Args:
            last_frame_idx: Last frame index in the video
            
        Returns:
            Boolean indicating if a final gesture was detected
        """
        if not self.gesture_active:
            return False
        
        # Process the final gesture candidate
        return self._process_gesture_candidate(last_frame_idx)
    
    def get_events(self):
        """Get the list of confirmed gesture events."""
        return self.confirmed_events
    
    def get_buckets(self):
        """Get the gesture bucket statistics."""
        return self.gesture_buckets
    
    def get_stats(self, duration_sec, frames_with_face, frames_total):
        """
        Get comprehensive gesture statistics.
        
        Args:
            duration_sec: Total duration of the video in seconds
            frames_with_face: Number of frames where a face was detected
            frames_total: Total number of frames processed
            
        Returns:
            Dictionary with gesture statistics
        """
        gesture_amplitudes = [e.get("amplitude", 0.0) for e in self.confirmed_events if e.get("amplitude")]
        
        return {
            "total_events": len(self.confirmed_events),
            "duration_sec": duration_sec,
            "rate_per_min": (len(self.confirmed_events) * 60) / max(duration_sec, 1e-6),
            "amplitude_mean": float(np.mean(gesture_amplitudes)) if gesture_amplitudes else 0.0,
            "amplitude_p95": float(np.percentile(gesture_amplitudes, 95)) if gesture_amplitudes else 0.0,
            "frames_with_face": frames_with_face,
            "frames_total": frames_total
        }


def run_analysis_pipeline(video_path: str) -> Dict[str, Any]:
    t0 = time.time()
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        logger.warning("Could not open video: %s", video_path)
        return {
            "frames_total": 0,
            "frames_with_face": 0,
            "fps": 0.0,
            "duration_sec": 0.0,
            "dropped_frames_pct": 0.0,
            "gesture_events": 0,
            "events": [],
            "media": {},
        }

    fps = float(cap.get(cv2.CAP_PROP_FPS) or 0.0)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    duration_sec = (frame_count / fps) if (fps and frame_count) else 0.0

    frames_total = 0
    frames_with_face = 0
    gesture_events = 0
    events = []
    dropped_frames_pct = 0.0
    media = {}

    # Accumulators for nonverbal metrics
    face_center_dist_sum = 0.0
    head_motion_sum = 0.0
    hand_motion_magnitude_sum = 0.0
    prev_head = None
    prev_hands = None
    
    # New accumulators for improved metrics
    posture_openness_sum = 0.0
    expression_variability_sum = 0.0
    gesture_motion_threshold = 0.02  # More sensitive threshold for gesture detection
    
    # Gesture event detection configuration
    gesture_detector = GestureDetector()
    gesture_detector.set_fps(fps)
    
    # Gesture event detection constants
    MIN_AMP = float(os.getenv("SPEECHUP_GESTURE_MIN_AMP", "0.18"))
    MIN_DUR_S = float(os.getenv("SPEECHUP_GESTURE_MIN_DUR", "0.08"))
    COOLDOWN_S = float(os.getenv("SPEECHUP_GESTURE_COOLDOWN", "0.25"))
    REQUIRE_FACE = os.getenv("SPEECHUP_GESTURE_REQUIRE_FACE", "1").lower() in ("1", "true", "True", "yes", "on")
    HYST_LOW = gesture_detector.hyst_low  # Hysteresis low threshold
    
    # Initialize events list for real-time gesture detection (backward compat)
    events = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        idx += 1
        frames_total += 1
        if idx % sample_every != 0:
            continue

        sampled_frames += 1
        h, w = frame.shape[:2]
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # Face detection
        face_res = mp_face.process(rgb)
        if face_res.detections:
            frames_with_face += 1
            bbox = face_res.detections[0].location_data.relative_bounding_box
            cx = bbox.xmin + bbox.width / 2.0
            cy = bbox.ymin + bbox.height / 2.0
            face_center_dist_sum += float(np.hypot(cx - 0.5, cy - 0.5))

        # Holistic analysis (pose + hands)
        hol = mp_holistic.process(rgb)
        
        # Face mesh for expression analysis
        face_mesh_res = mp_face_mesh.process(rgb)

        # Posture analysis
        if hol.pose_landmarks:
            posture_openness_val = compute_posture_openness(hol.pose_landmarks, w)
            posture_openness_sum += posture_openness_val
            
            # Head motion tracking
            nose = hol.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]
            head = np.array([nose.x, nose.y], dtype=np.float32)
            if prev_head is not None:
                head_motion_sum += float(np.linalg.norm(head - prev_head))
            prev_head = head

        # Expression analysis
        if face_mesh_res.multi_face_landmarks:
            face_landmarks = face_mesh_res.multi_face_landmarks[0]
            expression_var = compute_expression_variability(face_landmarks)
            expression_variability_sum += expression_var

        # Hand motion and gesture detection with windowing FSM
        left = hol.left_hand_landmarks
        right = hol.right_hand_landmarks
        
        # Consider one hand if only one is visible (left or right)
        if left or right:
            # Initialize hand positions
            lx, ly = 0.0, 0.0
            rx, ry = 0.0, 0.0
            
            if left:
                lw = left.landmark[mp.solutions.holistic.HandLandmark.WRIST]
                lx, ly = lw.x, lw.y
            if right:
                rw = right.landmark[mp.solutions.holistic.HandLandmark.WRIST]
                rx, ry = rw.x, rw.y
            
            # Create hands array with current positions
            hands = np.array([lx, ly, rx, ry], dtype=np.float32)
            
            if prev_hands is not None:
                # Calculate motion delta
                delta = float(np.linalg.norm(hands - prev_hands))
                hand_motion_magnitude_sum += delta
                
                # Compute current time in seconds for this frame
                t_sec = idx / (fps or 30.0)
                
                # Store motion magnitude and face presence for gesture detection
                gesture_detected = gesture_detector.track_frame(delta, bool(face_res.detections), idx)
                
                if gesture_detected:
                    events.append(gesture_detector.get_events()[-1])
            
            prev_hands = hands

    cap.release()
    
    # Process any remaining active gesture window
    if gesture_detector.gesture_active:
        gesture_detector.finalize(idx - 1)
        events.extend(gesture_detector.get_events())
    
    # Compute derived metrics
    mean_face_center_dist = face_center_dist_sum / max(frames_with_face, 1)
    normalized_head_motion = head_motion_sum / max(frames_with_face, 1)
    hand_motion_magnitude_avg = hand_motion_magnitude_sum / max(sampled_frames, 1)
    
    # New nonverbal metrics
    posture_openness = posture_openness_sum / max(sampled_frames, 1)
    expression_variability = expression_variability_sum / max(sampled_frames, 1)
    
    # Comprehensive gesture statistics using confirmed events
    gesture_stats = gesture_detector.get_stats(duration_sec, frames_with_face, frames_total)
    
    # Calculate coverage percentage across 5-second buckets
    total_buckets = max(1, int(duration_sec // 5) + 1)
    buckets_with_events = sum(1 for count in gesture_detector.get_buckets().values() if count > 0)
    coverage_pct = (buckets_with_events / total_buckets) * 100.0
    
    # Gesture rate per minute (for backward compatibility)
    gesture_rate_per_min = gesture_stats["rate_per_min"]
    
    # Gaze screen percentage (inverted face center distance)
    gaze_screen_pct = clamp(1.0 - (mean_face_center_dist / 0.5), 0.0, 1.0)
    
    # Head stability (inverted head motion)
    head_stability = clamp(1.0 - (4.0 * normalized_head_motion), 0.0, 1.0)
    
    # Gesture amplitude: normalize hand motion magnitude to [0,1]
    # Use 0.1 as "big gesture" baseline for normalization
    gesture_amplitude = clamp(hand_motion_magnitude_avg / 0.1, 0.0, 1.0)
    
    # Engagement metric: 60% gesture rate + 40% gesture amplitude
    # Normalize gesture rate by assuming 10 gestures per minute = 1.0
    gesture_rate_norm = clamp(gesture_rate_per_min / 10.0, 0.0, 1.0)
    engagement = 0.6 * gesture_rate_norm + 0.4 * gesture_amplitude
    
    # Improve posture_openness: clamp maximum to 0.95 to avoid saturation
    posture_openness = clamp(posture_openness, 0.0, 0.95)
    
    # Ensure all nonverbal metrics are normalized to [0,1]
    expression_variability = clamp(expression_variability, 0.0, 1.0)
    gaze_screen_pct = clamp(gaze_screen_pct, 0.0, 1.0)
    head_stability = clamp(head_stability, 0.0, 1.0)
    gesture_amplitude = clamp(gesture_amplitude, 0.0, 1.0)
    engagement = clamp(engagement, 0.0, 1.0)

    dropped_frames_pct = 0.0  # Placeholder for future implementation

    # Event truncation configuration
    MAX_EVENTS = Config.MAX_EVENTS
    
    proc = {
        "frames_total": frames_total,
        "frames_with_face": frames_with_face,
        "fps": fps,
        "duration_sec": duration_sec,
        "dropped_frames_pct": dropped_frames_pct,
        "gesture_events": gesture_events,
        "events": events[:MAX_EVENTS],  # Use confirmed events, truncate only at end
        "gesture_stats": gesture_stats,  # Include comprehensive stats
        "media": media,
        # Accumulators for derived metrics
        "face_center_dist_sum": face_center_dist_sum,
        "head_motion_sum": head_motion_sum,
        "hand_motion_magnitude_sum": hand_motion_magnitude_sum,
        # Derived metrics
        "mean_face_center_dist": mean_face_center_dist,
        "normalized_head_motion": normalized_head_motion,
        "hand_motion_magnitude_avg": hand_motion_magnitude_avg,
        "posture_openness": posture_openness,
        "expression_variability": expression_variability,
        "gesture_rate_per_min": gesture_rate_per_min,
        "gaze_screen_pct": gaze_screen_pct,
        "head_stability": head_stability,
        "gesture_amplitude": gesture_amplitude,
        "engagement": engagement,
        # Initialize audio structures with defaults
        "verbal": {
            "wpm": 0.0,
            "articulation_rate_sps": 0.0,
            "fillers_per_min": 0.0,
            "filler_counts": {},
            "avg_pause_sec": 0.0,
            "pause_rate_per_min": 0.0,
            "long_pauses": [],
            "pronunciation_score": 0.0,
            "stt_confidence": 0.0,
            "transcript_short": None,
        },
        "prosody": {
            "pitch_mean_hz": 0.0,
            "pitch_range_semitones": 0.0,
            "pitch_cv": 0.0,
            "energy_cv": 0.0,
            "rhythm_consistency": 0.0,
        },
    }
    
    # Log comprehensive gesture statistics
    logger.info(
        "GESTURES -> events=%d, rate_per_min=%.2f, amp_mean=%.3f, amp_p95=%.3f, face_ratio=%.3f",
        gesture_stats["total_events"], gesture_stats["rate_per_min"], 
        gesture_stats["amplitude_mean"], gesture_stats["amplitude_p95"],
        (gesture_stats["frames_with_face"] / max(gesture_stats["frames_total"], 1)) if gesture_stats["frames_total"] else 0.0
    )
    
    logger.info(
        "PIPELINE END: frames_total=%s, frames_with_face=%s, duration_sec=%.2f, fps=%.1f, "
        "gesture_events=%s, posture_openness=%.3f, expression_variability=%.3f, "
        "gesture_rate_per_min=%.2f, gaze_screen_pct=%.3f, head_stability=%.3f, "
        "gesture_amplitude=%.3f, engagement=%.3f",
        frames_total, frames_with_face, duration_sec, fps, gesture_events,
        posture_openness, expression_variability, gesture_rate_per_min, 
        gaze_screen_pct, head_stability, gesture_amplitude, engagement
    )
    
    # Audio processing for pause metrics (gated by SPEECHUP_USE_AUDIO)
    # Flags con default ON
    use_audio   = Config.USE_AUDIO
    use_asr     = Config.USE_ASR
    use_prosody = Config.USE_PROSODY

    logger.info(
        "FEATURES (effective) -> USE_AUDIO=%s USE_ASR=%s USE_PROSODY=%s",
        use_audio, use_asr, use_prosody
    )

    wav_path = None
    segments = []
    
    if use_audio or use_prosody:
        t_audio0 = time.time()
        try:
            # Extract audio from video
            wav_path = extract_wav_mono_16k(video_path)
            
            if wav_path:
                # Compute VAD segments
                try:
                    segments = compute_vad_segments(wav_path) or []
                except Exception as e:
                    logger.warning("VAD failed: %s", e)
                    segments = []
                
                # Compute pause metrics
                pause_metrics = compute_pause_metrics(segments, duration_sec)
                
                # Update verbal metrics with pause data
                proc["verbal"].update({
                    "avg_pause_sec": pause_metrics.get("avg_pause_sec", 0.0),
                    "pause_rate_per_min": pause_metrics.get("pause_rate_per_min", 0.0),
                    "long_pauses": pause_metrics.get("long_pauses", []),
                })
                
                # Prosody processing (defaults already initialized)

                if use_prosody:
                    t_pro0 = time.time()
                    try:
                        prosody = compute_prosody_metrics_from_path(wav_path, segments)
                        proc["prosody"].update({
                            "pitch_mean_hz": float(prosody.get("pitch_mean_hz", 0.0)),
                            "pitch_range_semitones": float(prosody.get("pitch_range_semitones", 0.0)),
                            "pitch_cv": float(prosody.get("pitch_cv", 0.0)),
                            "energy_cv": float(prosody.get("energy_cv", 0.0)),
                            "rhythm_consistency": float(prosody.get("rhythm_consistency", 0.0)),
                        })
                    except Exception as e:
                        logger.warning("Prosody failed, defaults applied: %s", e)
                    logger.info("Stage prosody done in %.1f ms", (time.time() - t_pro0) * 1000)
                
                proc["audio_available"] = True
                
                logger.info(
                    "Audio processing: %s segments, avg_pause_sec=%.2f, pause_rate_per_min=%.2f",
                    len(segments), pause_metrics.get("avg_pause_sec", 0.0), 
                    pause_metrics.get("pause_rate_per_min", 0.0)
                )
                logger.info("Stage audio+vad done in %.1f ms", (time.time() - t_audio0) * 1000)
                
                # Log prosody metrics if enabled
                if use_prosody and proc["prosody"].get("pitch_mean_hz", 0.0) > 0:
                    logger.info(
                        "Prosody metrics: pitch_mean=%.1fHz, range=%.1fst, pitch_cv=%.2f, "
                        "energy_cv=%.2f, rhythm=%.2f",
                        proc["prosody"].get("pitch_mean_hz", 0.0),
                        proc["prosody"].get("pitch_range_semitones", 0.0),
                        proc["prosody"].get("pitch_cv", 0.0),
                        proc["prosody"].get("energy_cv", 0.0),
                        proc["prosody"].get("rhythm_consistency", 0.0)
                    )
                
                # ASR processing (defaults + guarded)
                asr_error = None
                if (use_audio or use_asr) and wav_path:
                    t_asr0 = time.time()
                    try:
                        # Get VAD speech duration if available
                        speech_dur_sec = sum((e.get("end", 0.0) - e.get("start", 0.0)) for e in segments if e.get("end", 0.0) > e.get("start", 0.0)) if segments else 0.0
                        logger.info("VAD segments: %s, computed speech_dur_sec: %.2fs", len(segments), speech_dur_sec)
                        asr_result = transcribe_wav(wav_path, lang=None) or {}
                        
                        # Use trimmed duration if VAD not available
                        if speech_dur_sec < 0.1:
                            speech_dur_sec = asr_result.get("duration_sec", 0.0)
                        
                        # Log ASR health info
                        logger.info(f"ASR enabled: model={os.getenv('SPEECHUP_ASR_MODEL','base')}, device={os.getenv('WHISPER_DEVICE','auto')}, duration={getattr(asr_result,'duration_sec',0.0)}s, used_window={asr_result.get('duration_sec',0.0)}s")
                        
                        if asr_result.get("ok"):
                            text = asr_result.get("text", "") or ""
                            dur  = float(asr_result.get("duration_sec") or duration_sec or 0.0)
                            
                            # WPM and fillers only if we have duration
                            wpm = compute_wpm(text, dur) if dur and dur > 0 else 0.0
                            fillers = detect_spanish_fillers(text) or {"fillers_per_min": 0.0, "filler_counts": {}}
                            fillers_pm = normalize_fillers_per_minute(fillers.get("fillers_per_min", 0.0), dur) if dur and dur > 0 else 0.0

                            # Transcript management configuration
                            include_full = Config.INCLUDE_FULL_TRANSCRIPT
                            max_preview = Config.TRANSCRIPT_PREVIEW_MAX
                            
                            full_text = asr_result.get("text", "") or ""
                            proc["verbal"].update({
                                "wpm": float(wpm),
                                "fillers_per_min": float(fillers_pm),
                                "filler_counts": fillers.get("filler_counts", {}),
                                "stt_confidence": float(asr_result.get("stt_confidence", 0.0)),
                                "transcript_len": len(full_text),
                                "transcript_short": full_text[:max_preview] if full_text else None,
                            })
                            
                            # Include full transcript if enabled
                            if include_full:
                                proc["verbal"]["transcript_full"] = full_text
                            
                            logger.info("ASR ok: dur=%.2fs wpm=%.1f stt_conf=%.2f", dur, wpm, proc["verbal"]["stt_confidence"])
                            
                            # Fill missing verbal metrics
                            syll_per_word_es = 2.3  # Spanish average syllables per word
                            proc["verbal"]["articulation_rate_sps"] = float(max(0.0, (wpm * syll_per_word_es) / 60.0))
                            proc["verbal"]["pronunciation_score"] = float(max(0.0, min(1.0, proc["verbal"].get("stt_confidence", 0.0))))
                            
                            # Derive long_pauses from VAD segments
                            if segments and len(segments) > 1:
                                long_pauses = []
                                for i in range(len(segments) - 1):
                                    gap_start = segments[i].get("end", 0.0)
                                    gap_end = segments[i + 1].get("start", 0.0)
                                    gap_duration = gap_end - gap_start
                                    
                                    if gap_duration >= LONG_PAUSE_S:
                                        long_pauses.append({
                                            "start": float(gap_start),
                                            "end": float(gap_end),
                                            "duration": float(gap_duration)
                                        })
                                
                                if long_pauses:
                                    proc["verbal"]["long_pauses"] = long_pauses
                            
                            # Apply WPM smoothing for videos >= 20s
                            if duration_sec >= 20.0:
                                # For longer videos, apply 3-point moving average smoothing
                                # This is a simplified approach - in a real implementation you might
                                # want to segment the audio and compute WPM per segment
                                wpm_smoothed = wpm  # For now, keep as is since we don't have segment-level WPM
                                logger.info(f"WPM smoothing applied for video >=20s: raw={wpm:.1f}, smoothed={wpm_smoothed:.1f}")
                            
                            if os.getenv("SPEECHUP_DEBUG_ASR", "0") == "1":
                                logger.info(f"ASR transcript snippet: {text[:120]}")
                                logger.info(f"ASR transcript_short: {asr_result.get('transcript_short', '')}")
                        else:
                            asr_error = asr_result.get("error", "asr_not_ok")
                            logger.warning("ASR not ok: %s", asr_error)
                    except Exception as e:
                        asr_error = str(e)
                        logger.exception("ASR stage failed: %s", e)
                    logger.info("Stage ASR done in %.1f ms", (time.time() - t_asr0) * 1000)
                else:
                    if not (use_audio or use_asr):
                        logger.info("ASR skipped (flags off)")
                    elif not wav_path:
                        logger.info("ASR skipped (no wav_path)")
                
            else:
                logger.info("Audio extraction failed, skipping audio processing")
                proc["audio_available"] = False
                
        except Exception as e:
            logger.warning(f"Audio processing failed: {e}")
            proc["audio_available"] = False
        finally:
            # Always cleanup temporary WAV file
            if wav_path and os.path.exists(wav_path):
                try:
                    os.remove(wav_path)
                except Exception:
                    pass
    else:
        logger.info("Audio processing disabled (flags off)")
        
    # Log skipped stages explicitly
    if not use_prosody:
        logger.info("Prosody skipped (flag off)")
    if not (use_audio or use_asr):
        logger.info("ASR skipped (flags off)")
    
    # Compute dynamic scores based on analysis results
    try:
        proc["scores"] = compute_scores(proc)
        logger.info("Dynamic scores computed successfully")
    except Exception as e:
        logger.warning("Score computation failed, using defaults: %s", e)
        # Fallback to default scores if computation fails
        proc["scores"] = {
            "fluency": 65,
            "clarity": 65,
            "delivery_confidence": 65,
            "pronunciation": 65,
            "pace": 65,
            "engagement": 65,
        }
    
    # Surface ASR error and gesture statistics in quality block
    q = proc.setdefault("quality", {})
    dbg = q.setdefault("debug", {})
    
    # Asegurarse de que asr_error esté definida antes de usarla
    if 'asr_error' in locals() and asr_error:
        dbg["asr_error"] = asr_error
    
    # Add comprehensive gesture diagnostics to debug
    dbg.update({
        "gesture_events_total": gesture_stats["total_events"],
        "gesture_candidates_total": gesture_detector.gesture_candidates,
        "gesture_events_returned": min(len(gesture_detector.get_events()), Config.MAX_EVENTS),
        "gesture_buckets_5s": gesture_detector.get_buckets(),
        "coverage_pct": float(coverage_pct),
        "last_frame_ts": float(duration_sec),
        "face_present_ratio": float(frames_with_face) / max(frames_total, 1),
        "frames_with_face": gesture_stats["frames_with_face"],
        "max_events_config": Config.MAX_EVENTS,
        "gesture_params": {
            "min_amp": MIN_AMP,
            "min_dur": MIN_DUR_S,
            "cooldown": COOLDOWN_S,
            "require_face": REQUIRE_FACE,
            "hyst_low": HYST_LOW,
            "hyst_low_mult": HYST_LOW_MULT,
            "max_seg_s": MAX_SEG_S
        }
    })
    
    # Log gesture detection summary
    last_event_time = max([e.get("end_t", 0.0) for e in gesture_detector.get_events()], default=0.0)
    logger.info(
        "GESTURES -> total=%d, candidates=%d, rate=%.2f/min, coverage=%.1f%%, last_event=%.1fs",
        len(gesture_detector.get_events()),
        gesture_detector.gesture_candidates,
        gesture_stats["rate_per_min"],
        coverage_pct,
        last_event_time
    )
    
    # Log gesture parameters
    logger.info(
        "GESTURE PARAMS -> min_amp=%.3f, low=%.3f (%.2fx), min_dur=%.3fs, cooldown=%.3fs, max_seg=%.1fs, require_face=%s",
        MIN_AMP,
        HYST_LOW,
        HYST_LOW_MULT,
        MIN_DUR_S,
        COOLDOWN_S,
        MAX_SEG_S,
        REQUIRE_FACE
    )
    
    logger.info("Total pipeline time: %.1f ms", (time.time() - t0) * 1000)
    return proc
