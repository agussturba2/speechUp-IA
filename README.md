# Oratory Feedback System

A modular video analysis system that provides automated feedback on public speaking, posture, and presence, with personalized recommendations.

## Overview

This API allows users to upload presentation or speech videos to receive detailed analysis and feedback on oratory aspects such as:

- Face presence and visibility
- Body posture
- Perceived confidence
- Speech rhythm and fluency
- Energy and pitch variation
- Personalized recommendations generated by AI

The system analyzes videos frame by frame, detecting faces and posture using MediaPipe, and generates quantitative metrics along with qualitative feedback. Additionally, it analyzes audio for speech patterns, emotion, and speaking rate.

## Architecture

```
/
├── api/                       # API endpoints and routing
│   ├── main.py                # FastAPI application entrypoint
│   ├── routers/               # API route definitions
│   │   ├── oratory.py         # Video analysis endpoints
│   │   └── video_processing.py # Video processing helpers
│   └── websockets/            # WebSocket handlers
│       └── video.py           # Real-time video analysis
├── audio/                     # Audio processing modules
│   ├── analyzer.py            # Main orchestrator class
│   ├── emotion.py             # Emotion analysis
│   ├── extractors.py          # Audio extraction from media
│   ├── processors.py          # Audio preprocessing
│   ├── prosody.py             # Pitch and energy analysis
│   ├── recognition.py         # Speech recognition
│   └── speech.py              # Speech segmentation
├── video/                     # Video processing modules
│   ├── analysis/              # Analysis algorithms
│   │   └── faces.py           # Face and posture detection
│   ├── extract_frames.py      # Frame extraction from videos
│   ├── inference.py           # High-level inference API
│   ├── metrics.py             # Metrics calculation
│   ├── pipeline.py            # Processing orchestration
│   └── realtime.py            # Realtime processing
├── utils/                     # Shared utilities
│   ├── cache.py               # Cache management
│   ├── gpu.py                 # GPU utilities
│   ├── image_compress.py      # Image compression
│   ├── logging.py             # Structured logging
│   └── math.py                # Math helpers
├── pyproject.toml             # Project metadata and dependencies
└── requirements.txt           # Pin dependencies for deployment
```

## Architecture Diagram

```
┌────────────────┐     ┌────────────────┐     ┌────────────────┐
│   Web Client   │◄────┤   FastAPI App  │◄────┤  OLLAMA LLM    │
└───────┬────────┘     └───────┬────────┘     └────────────────┘
        │                      │
        │      ┌───────────────┴──────────────┐
        │      │                              │
        │      ▼                              ▼
┌───────┴────────┐               ┌────────────────────┐
│   WebSockets   │               │    REST API        │
└───────┬────────┘               └──────────┬─────────┘
        │                                   │
        │                                   │
        ▼                                   ▼
┌─────────────────┐             ┌────────────────────┐
│ Realtime        │             │ Batch Processing   │
│ Processing      │             │ Pipeline           │
└────────┬────────┘             └──────────┬─────────┘
         │                                  │
   ┌─────┴──────────────────────────┬──────┘
   │                                 │
   ▼                                 ▼
┌─────────────┐               ┌─────────────────┐
│ Video        │               │ Audio           │
│ Analysis     │◄─────────────►│ Analysis        │
└──────┬──────┘               └────────┬────────┘
       │                               │
       ▼                               ▼
┌─────────────────┐          ┌──────────────────┐
│ MediaPipe       │          │ Speech           │
│ Vision Models   │          │ Processing       │
└─────────────────┘          └──────────────────┘
```

## Installation

1. Clone the repository:

```bash
git clone <repository-url>
cd <directory-name>
```

2. Set up a virtual environment (optional but recommended):

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:

```bash
pip install -e .
```

4. Install Ollama (required for LLM feedback generation):
   - Download and install from [https://ollama.ai/](https://ollama.ai/)
   - Ensure it's running with `ollama serve`

5. Download the necessary model:

```bash
ollama pull llama3
```

6. Optional: Install additional dependencies for enhanced audio analysis:

```bash
# For speech recognition (transcription):
pip install vosk

# For emotion detection:
pip install tensorflow tensorflow-hub
```

7. Optional: Download speech recognition model:

```python
# For Spanish model
import vosk
vosk.Model.download("es")

# For English model
vosk.Model.download("en")
```

## Usage

### Start the API

```bash
uvicorn api.main:app --reload
```

The API will be available at `http://localhost:8000`.

### Interactive Documentation

Access the interactive API documentation at `http://localhost:8000/docs` to test endpoints directly from your browser.

### Send a video for analysis

El endpoint `/feedback-oratoria` ha sido optimizado para mejorar significativamente la velocidad de procesamiento y reducir el consumo de recursos:

```bash
curl -X POST "http://localhost:8000/feedback-oratoria" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "video_file=@your-video.mp4" \
  -F "fps_sample=2"
```

#### Parámetros

- `video_file`: El archivo de video a analizar (obligatorio)
- `fps_sample`: Número de frames por segundo a muestrear (opcional, predeterminado: 1)

#### Optimizaciones de Rendimiento

El sistema incluye importantes optimizaciones que mejoran drásticamente el tiempo de procesamiento:

1. **Muestreo de frames**: Procesa solo un subconjunto de frames del video (configurable con el parámetro `fps_sample`)
2. **Inicialización única del modelo**: Los modelos de MediaPipe se inicializan una sola vez fuera de los bucles de procesamiento
3. **Procesamiento paralelo**: Utiliza ThreadPoolExecutor para procesar frames en paralelo
4. **Procesamiento asíncrono**: El análisis del video se ejecuta en un thread separado para evitar bloquear el servidor
5. **Métricas de rendimiento**: Se incluyen métricas detalladas en la respuesta

Gracias a estas mejoras, videos que antes tomaban 10 minutos en procesar ahora se completan en una fracción de ese tiempo.

### Real-time Streaming via WebSocket

The `/feedback-realtime` endpoint provides live feedback during video recording or playback.

1. Conéctate al WebSocket:
   ```python
   import websockets, asyncio, cv2, struct

   async def stream(video_path):
       uri = "ws://localhost:8000/feedback-realtime"
       async with websockets.connect(uri) as ws:
           cap = cv2.VideoCapture(video_path)
           # Enviar frames
           while True:
               ret, frame = cap.read()
               if not ret:
                   break
               _, buf = cv2.imencode('.jpg', frame)
               await ws.send(buf.tobytes())
               # Recibir feedback (opcional timeout corto)
               try:
                   msg = await asyncio.wait_for(ws.recv(), 0.01)
                   print(msg)
               except asyncio.TimeoutError:
                   pass
           cap.release()
   asyncio.run(stream("video.mp4"))
   ```

2. Envío de audio opcional
   - El servidor acepta chunks de audio PCM mono 16 kHz de 1–2 s.
   - Cada mensaje binario de audio debe comenzar con el prefijo ASCII `"AUD"` (3 bytes) seguido de los datos PCM Little-Endian (`int16`). Ejemplo:
     ```python
     pcm_bytes = ...  # bytes de audio 16 kHz int16
     await ws.send(b"AUD" + pcm_bytes)
     ```
   - Cuando el servidor acumula ≈1 s de audio, calculará métricas de prosodia, pausas, muletillas y emoción y las agregará en el campo `audio_metrics` del JSON de feedback.

Para un ejemplo completo, revisa `test_realtime_client.py`, que muestra cómo enviar frames y leer la respuesta en tiempo real.

```bash
curl -X POST "http://localhost:8000/feedback-oratoria" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@tu-video.mp4" \
  -F "buffer_seconds=2" \
  -F "width=160" \
  -F "height=160" \
  -F "min_detection_confidence=0.6"
```

## Configuración del LLM (Modelo de Lenguaje)

El sistema utiliza Ollama para ejecutar un modelo de lenguaje local que proporciona feedback personalizado sobre oratoria.

### Parámetros configurables

En el archivo `video_processor/llm_feedback.py`, puedes ajustar:

1. **Modelo a utilizar**:
   - Por defecto: `llama3`
   - Para cambiar: modifica el parámetro `model` en la función `generar_feedback_llm`

2. **Tiempo de espera**:
   - Por defecto: 60 segundos
   - Para cambiar: modifica el parámetro `timeout` en la función `generar_feedback_llm`

3. **Prompt personalizado**:
   - El prompt está predefinido en la función `generar_feedback_llm`
   - Puedes modificarlo para que el feedback se adapte a tus necesidades específicas

### Ejemplo de uso directo del LLM

```python
from video_processor import generar_feedback_llm

# Crear métricas de ejemplo
metricas = {
    "fluency": 0.85,
    "clarity": 0.75,
    "pace": 0.8,
    "confidence": 0.7,
    "pronunciation": 0.8,
    "engagement": 0.75
}

# Generar feedback personalizado
feedback = generar_feedback_llm(
    metricas,
    {"porcentaje_frames_con_rostro": 85, "porcentaje_posturas_buenas": 75},
    ["Buena postura general", "Mantiene contacto visual"],
    ["Mejora la proyección de voz"],
    []
)

print(feedback)
```

## Detalles de Implementación

### Proceso de análisis

1. **Extracción de frames**:
   - El video se divide en buffers de N segundos (configurable)
   - Cada frame se redimensiona al tamaño especificado (default: 160x160)

2. **Detección de rostro y postura**:
   - MediaPipe se utiliza para detectar rostros en cada frame
   - Se analiza la postura corporal mediante la posición relativa de hombros y nariz

3. **Cálculo de métricas**:
   - Porcentaje de frames con rostro detectado
   - Porcentaje de frames con buena postura
   - Métricas simuladas: ritmo, confianza, pronunciación, engagement

4. **Generación de feedback**:
   - Se crea feedback específico por segmentos de tiempo
   - Se genera un resumen global con puntos fuertes y áreas de mejora
   - El LLM proporciona recomendaciones personalizadas en lenguaje natural

### Endpoints disponibles

- `GET /`: Verificación del estado de la API
- `POST /feedback-oratoria`: Enviar video y recibir análisis completo

## Respuesta de la API

La API devuelve un objeto JSON con:

- `duration`: Duración del video analizado
- `score`: Puntuación global (0-1)
- `metrics`: Métricas individuales
- `feedbacks`: Análisis por segmentos de tiempo
- `resumen_global`: Estadísticas generales
- `puntos_a_mejorar`: Sugerencias específicas
- `puntos_debiles`: Áreas críticas para trabajar
- `custom_feedback`: Recomendaciones en lenguaje natural
- `custom_feedback_llm`: Feedback elaborado por el LLM

## Optimizaciones de Rendimiento

El sistema implementa múltiples optimizaciones técnicas para procesar videos de manera eficiente, reduciendo significativamente los tiempos de procesamiento y el consumo de recursos.

### Aceleración por GPU

El sistema detecta y aprovecha automáticamente la GPU cuando está disponible:

```python
# Configuración automática de GPU
USE_GPU = False
try:
    if cv2.cuda.getCudaEnabledDeviceCount() > 0:
        USE_GPU = True
        print(f"GPU disponible para OpenCV: {cv2.cuda.printCudaDeviceInfo(0)}")
except:
    print("GPU no disponible para OpenCV")
```

- **Detección automática:** Identifica si hay GPU disponible para OpenCV y MediaPipe/TensorFlow
- **Procesamiento mixto:** Ejecuta operaciones intensivas en GPU y el resto en CPU
- **Modelo adaptativo:** Utiliza modelos más complejos cuando hay GPU disponible
- **Fallback seguro:** Si ocurre un error con GPU, automáticamente vuelve a usar CPU

### Sistema de Caché Multinivel

Implementa un sistema avanzado de caché en disco para evitar reprocesamiento:

```python
# Configuración del sistema de caché
cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'cache')
os.makedirs(cache_dir, exist_ok=True)
results_cache = Cache(cache_dir)
```

- **Caché de frames:** Almacena resultados de frames individuales usando hashing eficiente
- **Caché de buffers:** Guarda resultados de segmentos completos de video
- **Caché de tareas:** Almacena resultados de videos completos con sus configuraciones
- **Gestión automática:** Expiración configurable (24h por defecto) para evitar crecimiento excesivo
- **Hashing optimizado:** Usa versiones reducidas de frames para generar hashes rápidos y precisos

### Compresión Inteligente

Procesa diferentes regiones de cada frame con distinta calidad para optimizar recursos:

```python
def apply_smart_compression(frame, face_locations=None):
    # Si no hay rostros detectados, comprimir todo el frame
    if not face_locations:
        return cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)
    
    # Crear una máscara para áreas faciales
    mask = np.zeros(frame.shape[:2], dtype=np.uint8)
    
    # Marcar áreas faciales como 255 (blanco) en la máscara
    for x1, y1, x2, y2 in face_locations:
        # Expandir un poco el área para incluir más contexto
        padding = int(max(x2-x1, y2-y1) * 0.2)
        mask[y1-padding:y2+padding, x1-padding:x2+padding] = 255
```

- **Preservación selectiva:** Mantiene alta resolución solo en áreas faciales
- **Compresión agresiva:** Reduce significativamente la resolución en áreas no importantes
- **Máscara adaptativa:** Se ajusta dinámicamente según la posición de los rostros
- **Padding inteligente:** Incluye contexto suficiente alrededor de las áreas de interés

### Paralelización Optimizada

Adapta dinámicamente la paralelización según los recursos disponibles:

```python
# Determinar número óptimo de workers
if max_workers is None:
    cpu_count = os.cpu_count() or 4
    available_memory = psutil.virtual_memory().available / (1024 * 1024 * 1024)  # GB
    memory_based_workers = int(max(1, min(available_memory / 0.5, cpu_count)))
    max_workers = memory_based_workers
```

- **Workers dinámicos:** Ajusta el número de threads según CPU y memoria disponible
- **Procesamiento por lotes:** Divide grandes conjuntos de frames en lotes manejables
- **Balance óptimo:** Evita saturar la memoria o el CPU con demasiados threads
- **Estimación de recursos:** Calcula aproximadamente 0.5GB por worker para prevenir problemas

### Optimización de Memoria

Implementa múltiples estrategias para minimizar el consumo de memoria:

```python
# Liberar memoria explícitamente
all_buffers[i] = None

# Forzar recolector de basura periódicamente
if i % 5 == 0 or i == len(all_buffers) - 1:
    import gc
    gc.collect()
```

- **Liberación explícita:** Descarta datos procesados inmediatamente
- **Recolección de basura forzada:** Ejecuta manualmente el GC en momentos estratégicos
- **Pre-asignación:** Crea arrays de tamaño conocido para evitar expansiones dinámicas
- **Monitoreo continuo:** Registra y reporta el uso de memoria durante el procesamiento

### Muestreo Configurable de Frames

Permite procesar solo una fracción de los frames para análisis más rápido:

```python
# Muestrear frames
if fps_sample and fps_sample > 1:
    sampled_buffer = [buffer[i] for i in range(0, len(buffer), fps_sample)]
    sampled_idx = [indices[i] for i in range(0, len(indices), fps_sample)]
```

- **Muestreo adaptable:** Procesa 1 de cada N frames según configuración
- **Preservación de índices:** Mantiene la referencia temporal correcta
- **Escalabilidad:** Ideal para videos largos donde no se requiere análisis frame a frame
- **Configuración flexible:** Permite ajustar la densidad del muestreo según necesidades

### Optimizaciones Numéricas

Usa Numba para acelerar cálculos matemáticos repetitivos:

```python
@jit(nopython=False, forceobj=True)
def calculate_posture_score(l_shoulder_x, r_shoulder_x, nose_x):
    """Calcula puntuación de postura optimizada con Numba"""
    return abs((l_shoulder_x + r_shoulder_x)/2 - nose_x) < 0.08
```

- **Compilación JIT:** Convierte funciones Python en código optimizado para CPU
- **Reducción de overhead:** Minimiza el costo de llamadas a funciones repetitivas
- **Vectorización:** Aprovecha instrucciones SIMD cuando están disponibles

### Métricas de Rendimiento

Registra y devuelve métricas detalladas de rendimiento:

```python
performance_metrics = {
    "processing_time_seconds": processing_time,
    "frames_per_second": total_frames_processed / processing_time,
    "initial_memory_mb": initial_memory,
    "peak_memory_mb": peak_memory,
    "memory_increase_mb": peak_memory - initial_memory,
    "total_frames_processed": total_frames_processed,
    "gpu_used": USE_GPU,
    "fps_sample": fps_sample if fps_sample else 1
}
```

- **Tiempos precisos:** Mide duración total y velocidad de procesamiento (FPS)
- **Monitoreo de memoria:** Registra uso inicial, pico y crecimiento
- **Estadísticas de uso:** Indica si se usó GPU y qué tasa de muestreo se aplicó
- **Diagnóstico:** Facilita la identificación de cuellos de botella

### Configuración Avanzada

Las optimizaciones pueden ajustarse mediante parámetros:

| Parámetro | Descripción | Valor por defecto |
|-----------|-------------|-------------------|
| `fps_sample` | Procesa 1 de cada N frames | `None` (todos) |
| `max_workers` | Límite de threads paralelos | Auto (basado en CPU/memoria) |
| `batch_size` | Tamaño de lotes para procesamiento | Auto (basado en workers) |
| `min_detection_confidence` | Umbral para detecciones | `0.6` |

### Ejemplo de uso

```python
from video_processor.inference import process_all_buffers

# Procesamiento estándar
feedbacks, faces, posture, metrics = process_all_buffers(all_buffers, all_indices)

# Procesamiento optimizado para videos largos
feedbacks, faces, posture, metrics = process_all_buffers(
    all_buffers, 
    all_indices,
    fps_sample=3,  # Procesa 1 de cada 3 frames
    max_workers=8  # Fuerza uso de 8 threads
)

# Acceder a métricas de rendimiento
print(f"Tiempo total: {metrics['processing_time_seconds']:.2f} segundos")
print(f"Velocidad: {metrics['frames_per_second']:.1f} FPS")
print(f"Memoria usada: {metrics['memory_increase_mb']:.1f} MB")
print(f"GPU utilizada: {'Sí' if metrics['gpu_used'] else 'No'}")
```

### Resultados esperados

Con estas optimizaciones, se puede esperar:

- **Reducción de tiempo:** Entre 70-90% para videos largos al usar GPU + muestreo
- **Menor consumo de memoria:** Hasta 60% menos comparado con la versión original
- **Mayor escalabilidad:** Capacidad para procesar videos mucho más largos
- **Procesamiento adaptativo:** Ajuste automático según recursos disponibles

## Mejoras y Extensiones Futuras

Algunas posibles mejoras adicionales para el proyecto:

1. **Análisis de audio**:
   - Incorporar procesamiento de audio para analizar tono, ritmo, y claridad del habla

2. **Interfaz web**:
   - Desarrollar una interfaz frontend para subir videos y visualizar resultados

3. **Más modelos de análisis**:
   - Incorporar detección de gestos y expresiones faciales
   - Análisis de la mirada y contacto visual

4. **Personalización del LLM**:
   - Fine-tuning del modelo para mejorar la calidad del feedback específico de oratoria
   - Soporte para múltiples idiomas

## Requisitos del sistema

- Python 3.8+
- FastAPI
- OpenCV
- MediaPipe
- Numpy
- Ollama (para generación de feedback con LLM)

## Licencia

[Especificar la licencia del proyecto]
